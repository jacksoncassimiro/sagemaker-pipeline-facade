{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "42b5e80b-ad1d-4335-a1f7-10a91127e3dc"
    }
   },
   "source": [
    "# Amazon SageMaker Batch Transform: Associate prediction results with their corresponding input records\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook. \n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-2/sagemaker_batch_transform|batch_transform_associate_predictions_with_input|Batch Transform - breast cancer prediction with lowel level SDK.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "42b5e80b-ad1d-4335-a1f7-10a91127e3dc"
    }
   },
   "source": [
    "_**Use SageMaker's XGBoost to train a binary classification model and for a list of tumors in batch file, predict if each is malignant**_\n",
    "\n",
    "_**It also shows how to use the input output joining / filter feature in Batch transform in details**_\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "## Background\n",
    "This purpose of this notebook is to train a model using SageMaker's XGBoost and UCI's breast cancer diagnostic data set to illustrate at how to run batch inferences and how to use the Batch Transform I/O join feature. UCI's breast cancer diagnostic data set is available at https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29. The data set is also available on Kaggle at https://www.kaggle.com/uciml/breast-cancer-wisconsin-data. The purpose here is to use this data set to build a predictve model of whether a breast mass image indicates benign or malignant tumor. \n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "* The SageMaker role arn used to give training and batch transform access to your data. The snippet below will use the same role used by your SageMaker notebook instance. Otherwise, specify the full ARN of a role with the SageMakerFullAccess policy attached.\n",
    "* The S3 bucket that you want to use for training and storing model objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "isConfigCell": true,
    "nbpresent": {
     "id": "6427e831-8f89-45c0-b150-0b134397d79a"
    },
    "tags": [
     "parameters"
    ],
    "ExecuteTime": {
     "end_time": "2023-12-18T16:52:02.198414831Z",
     "start_time": "2023-12-18T16:52:02.155871701Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "os.environ['AWS_DEFAULT_REGION'] = 'us-east-1'\n",
    "role = 'arn:aws:iam::898487103080:role/ml-experiments'\n",
    "\n",
    "bucket = 'turin-complete-experiments'\n",
    "prefix = \"breast-cancer\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "142777ae-c072-448e-b941-72bc75735d01"
    }
   },
   "source": [
    "---\n",
    "## Data sources\n",
    "\n",
    "> Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.\n",
    "\n",
    "> Breast Cancer Wisconsin (Diagnostic) Data Set [https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)].\n",
    "\n",
    "> _Also see:_ Breast Cancer Wisconsin (Diagnostic) Data Set [https://www.kaggle.com/uciml/breast-cancer-wisconsin-data].\n",
    "\n",
    "## Data preparation\n",
    "\n",
    "\n",
    "Let's download the data and save it in the local folder with the name data.csv and take a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbpresent": {
     "id": "f8976dad-6897-4c7e-8c95-ae2f53070ef5"
    },
    "ExecuteTime": {
     "end_time": "2023-12-18T15:39:22.223794184Z",
     "start_time": "2023-12-18T15:39:22.107707072Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "          id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n405   904971         B       10.940         18.59           70.39      370.0   \n316   894090         B       12.180         14.08           77.25      461.4   \n75   8610404         M       16.070         19.65          104.10      817.7   \n47     85715         M       13.170         18.66           85.98      534.6   \n553   924342         B        9.333         21.94           59.01      264.0   \n241   883539         B       12.420         15.04           78.61      476.5   \n492   914062         M       18.010         20.56          118.40     1007.0   \n166    87127         B       10.800          9.71           68.77      357.6   \n\n     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n405          0.10040           0.07460         0.04944             0.029320   \n316          0.07734           0.03212         0.01123             0.005051   \n75           0.09168           0.08424         0.09769             0.066380   \n47           0.11580           0.12310         0.12260             0.073400   \n553          0.09240           0.05605         0.03996             0.012820   \n241          0.07926           0.03393         0.01053             0.011080   \n492          0.10010           0.12890         0.11700             0.077620   \n166          0.09594           0.05736         0.02531             0.016980   \n\n     ...  radius_worst  texture_worst  perimeter_worst  area_worst  \\\n405  ...        12.400          25.58            82.76       472.4   \n316  ...        12.850          16.47            81.60       513.1   \n75   ...        19.770          24.56           128.80      1223.0   \n47   ...        15.670          27.95           102.80       759.4   \n553  ...         9.845          25.05            62.86       295.8   \n241  ...        13.200          20.37            83.85       543.4   \n492  ...        21.530          26.06           143.40      1426.0   \n166  ...        11.600          12.02            73.66       414.0   \n\n     smoothness_worst  compactness_worst  concavity_worst  \\\n405            0.1363            0.16440          0.14120   \n316            0.1001            0.05332          0.04116   \n75             0.1500            0.20450          0.28290   \n47             0.1786            0.41660          0.50060   \n553            0.1103            0.08298          0.07993   \n241            0.1037            0.07776          0.06243   \n492            0.1309            0.23270          0.25440   \n166            0.1436            0.12570          0.10470   \n\n     concave points_worst  symmetry_worst  fractal_dimension_worst  \n405               0.07887          0.2251                  0.07732  \n316               0.01852          0.2293                  0.06037  \n75                0.15200          0.2650                  0.06387  \n47                0.20880          0.3900                  0.11790  \n553               0.02564          0.2435                  0.07393  \n241               0.04052          0.2901                  0.06783  \n492               0.14890          0.3251                  0.07625  \n166               0.04603          0.2090                  0.07699  \n\n[8 rows x 32 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>diagnosis</th>\n      <th>radius_mean</th>\n      <th>texture_mean</th>\n      <th>perimeter_mean</th>\n      <th>area_mean</th>\n      <th>smoothness_mean</th>\n      <th>compactness_mean</th>\n      <th>concavity_mean</th>\n      <th>concave points_mean</th>\n      <th>...</th>\n      <th>radius_worst</th>\n      <th>texture_worst</th>\n      <th>perimeter_worst</th>\n      <th>area_worst</th>\n      <th>smoothness_worst</th>\n      <th>compactness_worst</th>\n      <th>concavity_worst</th>\n      <th>concave points_worst</th>\n      <th>symmetry_worst</th>\n      <th>fractal_dimension_worst</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>405</th>\n      <td>904971</td>\n      <td>B</td>\n      <td>10.940</td>\n      <td>18.59</td>\n      <td>70.39</td>\n      <td>370.0</td>\n      <td>0.10040</td>\n      <td>0.07460</td>\n      <td>0.04944</td>\n      <td>0.029320</td>\n      <td>...</td>\n      <td>12.400</td>\n      <td>25.58</td>\n      <td>82.76</td>\n      <td>472.4</td>\n      <td>0.1363</td>\n      <td>0.16440</td>\n      <td>0.14120</td>\n      <td>0.07887</td>\n      <td>0.2251</td>\n      <td>0.07732</td>\n    </tr>\n    <tr>\n      <th>316</th>\n      <td>894090</td>\n      <td>B</td>\n      <td>12.180</td>\n      <td>14.08</td>\n      <td>77.25</td>\n      <td>461.4</td>\n      <td>0.07734</td>\n      <td>0.03212</td>\n      <td>0.01123</td>\n      <td>0.005051</td>\n      <td>...</td>\n      <td>12.850</td>\n      <td>16.47</td>\n      <td>81.60</td>\n      <td>513.1</td>\n      <td>0.1001</td>\n      <td>0.05332</td>\n      <td>0.04116</td>\n      <td>0.01852</td>\n      <td>0.2293</td>\n      <td>0.06037</td>\n    </tr>\n    <tr>\n      <th>75</th>\n      <td>8610404</td>\n      <td>M</td>\n      <td>16.070</td>\n      <td>19.65</td>\n      <td>104.10</td>\n      <td>817.7</td>\n      <td>0.09168</td>\n      <td>0.08424</td>\n      <td>0.09769</td>\n      <td>0.066380</td>\n      <td>...</td>\n      <td>19.770</td>\n      <td>24.56</td>\n      <td>128.80</td>\n      <td>1223.0</td>\n      <td>0.1500</td>\n      <td>0.20450</td>\n      <td>0.28290</td>\n      <td>0.15200</td>\n      <td>0.2650</td>\n      <td>0.06387</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>85715</td>\n      <td>M</td>\n      <td>13.170</td>\n      <td>18.66</td>\n      <td>85.98</td>\n      <td>534.6</td>\n      <td>0.11580</td>\n      <td>0.12310</td>\n      <td>0.12260</td>\n      <td>0.073400</td>\n      <td>...</td>\n      <td>15.670</td>\n      <td>27.95</td>\n      <td>102.80</td>\n      <td>759.4</td>\n      <td>0.1786</td>\n      <td>0.41660</td>\n      <td>0.50060</td>\n      <td>0.20880</td>\n      <td>0.3900</td>\n      <td>0.11790</td>\n    </tr>\n    <tr>\n      <th>553</th>\n      <td>924342</td>\n      <td>B</td>\n      <td>9.333</td>\n      <td>21.94</td>\n      <td>59.01</td>\n      <td>264.0</td>\n      <td>0.09240</td>\n      <td>0.05605</td>\n      <td>0.03996</td>\n      <td>0.012820</td>\n      <td>...</td>\n      <td>9.845</td>\n      <td>25.05</td>\n      <td>62.86</td>\n      <td>295.8</td>\n      <td>0.1103</td>\n      <td>0.08298</td>\n      <td>0.07993</td>\n      <td>0.02564</td>\n      <td>0.2435</td>\n      <td>0.07393</td>\n    </tr>\n    <tr>\n      <th>241</th>\n      <td>883539</td>\n      <td>B</td>\n      <td>12.420</td>\n      <td>15.04</td>\n      <td>78.61</td>\n      <td>476.5</td>\n      <td>0.07926</td>\n      <td>0.03393</td>\n      <td>0.01053</td>\n      <td>0.011080</td>\n      <td>...</td>\n      <td>13.200</td>\n      <td>20.37</td>\n      <td>83.85</td>\n      <td>543.4</td>\n      <td>0.1037</td>\n      <td>0.07776</td>\n      <td>0.06243</td>\n      <td>0.04052</td>\n      <td>0.2901</td>\n      <td>0.06783</td>\n    </tr>\n    <tr>\n      <th>492</th>\n      <td>914062</td>\n      <td>M</td>\n      <td>18.010</td>\n      <td>20.56</td>\n      <td>118.40</td>\n      <td>1007.0</td>\n      <td>0.10010</td>\n      <td>0.12890</td>\n      <td>0.11700</td>\n      <td>0.077620</td>\n      <td>...</td>\n      <td>21.530</td>\n      <td>26.06</td>\n      <td>143.40</td>\n      <td>1426.0</td>\n      <td>0.1309</td>\n      <td>0.23270</td>\n      <td>0.25440</td>\n      <td>0.14890</td>\n      <td>0.3251</td>\n      <td>0.07625</td>\n    </tr>\n    <tr>\n      <th>166</th>\n      <td>87127</td>\n      <td>B</td>\n      <td>10.800</td>\n      <td>9.71</td>\n      <td>68.77</td>\n      <td>357.6</td>\n      <td>0.09594</td>\n      <td>0.05736</td>\n      <td>0.02531</td>\n      <td>0.016980</td>\n      <td>...</td>\n      <td>11.600</td>\n      <td>12.02</td>\n      <td>73.66</td>\n      <td>414.0</td>\n      <td>0.1436</td>\n      <td>0.12570</td>\n      <td>0.10470</td>\n      <td>0.04603</td>\n      <td>0.2090</td>\n      <td>0.07699</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 32 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('../datasets/wdbc.csv')\n",
    "\n",
    "# specify columns extracted from wbdc.names\n",
    "selected_columns = [\n",
    "    \"id\",\n",
    "    \"diagnosis\",\n",
    "    \"radius_mean\",\n",
    "    \"texture_mean\",\n",
    "    \"perimeter_mean\",\n",
    "    \"area_mean\",\n",
    "    \"smoothness_mean\",\n",
    "    \"compactness_mean\",\n",
    "    \"concavity_mean\",\n",
    "    \"concave points_mean\",\n",
    "    \"symmetry_mean\",\n",
    "    \"fractal_dimension_mean\",\n",
    "    \"radius_se\",\n",
    "    \"texture_se\",\n",
    "    \"perimeter_se\",\n",
    "    \"area_se\",\n",
    "    \"smoothness_se\",\n",
    "    \"compactness_se\",\n",
    "    \"concavity_se\",\n",
    "    \"concave points_se\",\n",
    "    \"symmetry_se\",\n",
    "    \"fractal_dimension_se\",\n",
    "    \"radius_worst\",\n",
    "    \"texture_worst\",\n",
    "    \"perimeter_worst\",\n",
    "    \"area_worst\",\n",
    "    \"smoothness_worst\",\n",
    "    \"compactness_worst\",\n",
    "    \"concavity_worst\",\n",
    "    \"concave points_worst\",\n",
    "    \"symmetry_worst\",\n",
    "    \"fractal_dimension_worst\",\n",
    "]\n",
    "\n",
    "data = data[selected_columns]\n",
    "\n",
    "# save the data\n",
    "data.to_csv(\"data.csv\", sep=\",\", index=False)\n",
    "\n",
    "data.sample(8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key observations:\n",
    "* The data has 569 observations and 32 columns.\n",
    "* The first field is the 'id' attribute that we will want to drop before batch inference and add to the final inference output next to the probability of malignancy.\n",
    "* Second field, 'diagnosis', is an indicator of the actual diagnosis ('M' = Malignant; 'B' = Benign).\n",
    "* There are 30 other numeric features that we will use for training and inferencing."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's replace the M/B diagnosis with a 1/0 boolean value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T15:41:10.882617763Z",
     "start_time": "2023-12-18T15:41:10.837719420Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "           id  diagnosis  radius_mean  texture_mean  perimeter_mean  \\\n458   9112594          0        13.00         25.13           82.61   \n447   9110944          0        14.80         17.66           95.88   \n454    911202          0        12.62         17.15           80.62   \n241    883539          0        12.42         15.04           78.61   \n334    897374          0        12.30         19.02           77.88   \n490  91376701          0        12.25         22.44           78.18   \n50     857343          0        11.76         21.60           74.72   \n235  88249602          0        14.03         21.25           89.79   \n\n     area_mean  smoothness_mean  compactness_mean  concavity_mean  \\\n458      520.2          0.08369           0.05073        0.012060   \n447      674.8          0.09179           0.08890        0.040690   \n454      492.9          0.08583           0.05430        0.029660   \n241      476.5          0.07926           0.03393        0.010530   \n334      464.4          0.08313           0.04202        0.007756   \n490      466.5          0.08192           0.05200        0.017140   \n50       427.9          0.08637           0.04966        0.016570   \n235      603.4          0.09070           0.06945        0.014620   \n\n     concave points_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n458             0.017620  ...         14.34          31.88            91.06   \n447             0.022600  ...         16.43          22.74           105.90   \n454             0.022720  ...         14.34          22.15            91.62   \n241             0.011080  ...         13.20          20.37            83.85   \n334             0.008535  ...         13.35          28.46            84.53   \n490             0.012610  ...         14.17          31.99            92.74   \n50              0.011150  ...         12.98          25.72            82.98   \n235             0.018960  ...         15.33          30.28            98.27   \n\n     area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n458       628.5            0.1218            0.10930          0.04462   \n447       829.5            0.1226            0.18810          0.20600   \n454       633.5            0.1225            0.15170          0.18870   \n241       543.4            0.1037            0.07776          0.06243   \n334       544.3            0.1222            0.09052          0.03619   \n490       622.9            0.1256            0.18040          0.12300   \n50        516.5            0.1085            0.08615          0.05523   \n235       715.5            0.1287            0.15130          0.06231   \n\n     concave points_worst  symmetry_worst  fractal_dimension_worst  \n458               0.05921          0.2306                  0.06291  \n447               0.08308          0.3600                  0.07285  \n454               0.09851          0.3270                  0.07330  \n241               0.04052          0.2901                  0.06783  \n334               0.03983          0.2554                  0.07207  \n490               0.06335          0.3100                  0.08203  \n50                0.03715          0.2433                  0.06563  \n235               0.07963          0.2226                  0.07617  \n\n[8 rows x 32 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>diagnosis</th>\n      <th>radius_mean</th>\n      <th>texture_mean</th>\n      <th>perimeter_mean</th>\n      <th>area_mean</th>\n      <th>smoothness_mean</th>\n      <th>compactness_mean</th>\n      <th>concavity_mean</th>\n      <th>concave points_mean</th>\n      <th>...</th>\n      <th>radius_worst</th>\n      <th>texture_worst</th>\n      <th>perimeter_worst</th>\n      <th>area_worst</th>\n      <th>smoothness_worst</th>\n      <th>compactness_worst</th>\n      <th>concavity_worst</th>\n      <th>concave points_worst</th>\n      <th>symmetry_worst</th>\n      <th>fractal_dimension_worst</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>458</th>\n      <td>9112594</td>\n      <td>0</td>\n      <td>13.00</td>\n      <td>25.13</td>\n      <td>82.61</td>\n      <td>520.2</td>\n      <td>0.08369</td>\n      <td>0.05073</td>\n      <td>0.012060</td>\n      <td>0.017620</td>\n      <td>...</td>\n      <td>14.34</td>\n      <td>31.88</td>\n      <td>91.06</td>\n      <td>628.5</td>\n      <td>0.1218</td>\n      <td>0.10930</td>\n      <td>0.04462</td>\n      <td>0.05921</td>\n      <td>0.2306</td>\n      <td>0.06291</td>\n    </tr>\n    <tr>\n      <th>447</th>\n      <td>9110944</td>\n      <td>0</td>\n      <td>14.80</td>\n      <td>17.66</td>\n      <td>95.88</td>\n      <td>674.8</td>\n      <td>0.09179</td>\n      <td>0.08890</td>\n      <td>0.040690</td>\n      <td>0.022600</td>\n      <td>...</td>\n      <td>16.43</td>\n      <td>22.74</td>\n      <td>105.90</td>\n      <td>829.5</td>\n      <td>0.1226</td>\n      <td>0.18810</td>\n      <td>0.20600</td>\n      <td>0.08308</td>\n      <td>0.3600</td>\n      <td>0.07285</td>\n    </tr>\n    <tr>\n      <th>454</th>\n      <td>911202</td>\n      <td>0</td>\n      <td>12.62</td>\n      <td>17.15</td>\n      <td>80.62</td>\n      <td>492.9</td>\n      <td>0.08583</td>\n      <td>0.05430</td>\n      <td>0.029660</td>\n      <td>0.022720</td>\n      <td>...</td>\n      <td>14.34</td>\n      <td>22.15</td>\n      <td>91.62</td>\n      <td>633.5</td>\n      <td>0.1225</td>\n      <td>0.15170</td>\n      <td>0.18870</td>\n      <td>0.09851</td>\n      <td>0.3270</td>\n      <td>0.07330</td>\n    </tr>\n    <tr>\n      <th>241</th>\n      <td>883539</td>\n      <td>0</td>\n      <td>12.42</td>\n      <td>15.04</td>\n      <td>78.61</td>\n      <td>476.5</td>\n      <td>0.07926</td>\n      <td>0.03393</td>\n      <td>0.010530</td>\n      <td>0.011080</td>\n      <td>...</td>\n      <td>13.20</td>\n      <td>20.37</td>\n      <td>83.85</td>\n      <td>543.4</td>\n      <td>0.1037</td>\n      <td>0.07776</td>\n      <td>0.06243</td>\n      <td>0.04052</td>\n      <td>0.2901</td>\n      <td>0.06783</td>\n    </tr>\n    <tr>\n      <th>334</th>\n      <td>897374</td>\n      <td>0</td>\n      <td>12.30</td>\n      <td>19.02</td>\n      <td>77.88</td>\n      <td>464.4</td>\n      <td>0.08313</td>\n      <td>0.04202</td>\n      <td>0.007756</td>\n      <td>0.008535</td>\n      <td>...</td>\n      <td>13.35</td>\n      <td>28.46</td>\n      <td>84.53</td>\n      <td>544.3</td>\n      <td>0.1222</td>\n      <td>0.09052</td>\n      <td>0.03619</td>\n      <td>0.03983</td>\n      <td>0.2554</td>\n      <td>0.07207</td>\n    </tr>\n    <tr>\n      <th>490</th>\n      <td>91376701</td>\n      <td>0</td>\n      <td>12.25</td>\n      <td>22.44</td>\n      <td>78.18</td>\n      <td>466.5</td>\n      <td>0.08192</td>\n      <td>0.05200</td>\n      <td>0.017140</td>\n      <td>0.012610</td>\n      <td>...</td>\n      <td>14.17</td>\n      <td>31.99</td>\n      <td>92.74</td>\n      <td>622.9</td>\n      <td>0.1256</td>\n      <td>0.18040</td>\n      <td>0.12300</td>\n      <td>0.06335</td>\n      <td>0.3100</td>\n      <td>0.08203</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>857343</td>\n      <td>0</td>\n      <td>11.76</td>\n      <td>21.60</td>\n      <td>74.72</td>\n      <td>427.9</td>\n      <td>0.08637</td>\n      <td>0.04966</td>\n      <td>0.016570</td>\n      <td>0.011150</td>\n      <td>...</td>\n      <td>12.98</td>\n      <td>25.72</td>\n      <td>82.98</td>\n      <td>516.5</td>\n      <td>0.1085</td>\n      <td>0.08615</td>\n      <td>0.05523</td>\n      <td>0.03715</td>\n      <td>0.2433</td>\n      <td>0.06563</td>\n    </tr>\n    <tr>\n      <th>235</th>\n      <td>88249602</td>\n      <td>0</td>\n      <td>14.03</td>\n      <td>21.25</td>\n      <td>89.79</td>\n      <td>603.4</td>\n      <td>0.09070</td>\n      <td>0.06945</td>\n      <td>0.014620</td>\n      <td>0.018960</td>\n      <td>...</td>\n      <td>15.33</td>\n      <td>30.28</td>\n      <td>98.27</td>\n      <td>715.5</td>\n      <td>0.1287</td>\n      <td>0.15130</td>\n      <td>0.06231</td>\n      <td>0.07963</td>\n      <td>0.2226</td>\n      <td>0.07617</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 32 columns</p>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"diagnosis\"] = data[\"diagnosis\"].apply(lambda x: ((x == \"M\")) + 0)\n",
    "data.sample(8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the data as follows: 80% for training, 10% for validation and let's set 10% aside for our batch inference job. In addition, let's drop the 'id' field on the training set and validation set as 'id' is not a training feature. For our batch set however, we keep the 'id' feature. We'll want to filter it out prior to running our inferences so that the input data features match the ones of training set and then ultimately, we'll want to join it with inference result. We are however dropping the diagnosis attribute for the batch set since this is what we'll try to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T15:41:24.387635681Z",
     "start_time": "2023-12-18T15:41:24.364112446Z"
    }
   },
   "outputs": [],
   "source": [
    "# data split in three sets, training, validation and batch inference\n",
    "rand_split = np.random.rand(len(data))\n",
    "train_list = rand_split < 0.8\n",
    "val_list = (rand_split >= 0.8) & (rand_split < 0.9)\n",
    "batch_list = rand_split >= 0.9\n",
    "\n",
    "data_train = data[train_list].drop([\"id\"], axis=1)\n",
    "data_val = data[val_list].drop([\"id\"], axis=1)\n",
    "data_batch = data[batch_list].drop([\"diagnosis\"], axis=1)\n",
    "data_batch_noID = data_batch.drop([\"id\"], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ff9d10f9-b611-423b-80da-6dcdafd1c8b9"
    }
   },
   "source": [
    "Let's upload those data sets in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "nbpresent": {
     "id": "cd8e3431-79d9-40b6-91d1-d67cd61894e7"
    },
    "ExecuteTime": {
     "end_time": "2023-12-18T16:46:51.950187295Z",
     "start_time": "2023-12-18T16:46:49.502898643Z"
    }
   },
   "outputs": [],
   "source": [
    "s3_resource = boto3.Session().resource(\"s3\")\n",
    "\n",
    "train_file = \"train_data.csv\"\n",
    "data_train.to_csv(train_file, index=False, header=False)\n",
    "with open(train_file, \"rb\") as data:\n",
    "    s3_resource.Bucket(bucket).upload_fileobj(data, os.path.join(prefix, \"train\", train_file))\n",
    "\n",
    "validation_file = \"validation_data.csv\"\n",
    "data_val.to_csv(validation_file, index=False, header=False)\n",
    "with open(validation_file, \"rb\") as data:\n",
    "    s3_resource.Bucket(bucket).upload_fileobj(\n",
    "        data, os.path.join(prefix, \"validation\", validation_file)\n",
    "    )\n",
    "\n",
    "batch_file = \"batch_data.csv\"\n",
    "data_batch.to_csv(batch_file, index=False, header=False)\n",
    "with open(batch_file, \"rb\") as data:\n",
    "    s3_resource.Bucket(bucket).upload_fileobj(data, os.path.join(prefix, \"batch\", batch_file))\n",
    "\n",
    "batch_file_noID = \"batch_data_noID.csv\"\n",
    "data_batch_noID.to_csv(batch_file_noID, index=False, header=False)\n",
    "with open(batch_file_noID, \"rb\") as data:\n",
    "    s3_resource.Bucket(bucket).upload_fileobj(data, os.path.join(prefix, \"batch\", batch_file_noID))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "71cbcebd-a2a5-419e-8e50-b2bc0909f564"
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Training job and model creation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "bd113b8e-adc1-4091-a26f-a426149fe604"
    }
   },
   "source": [
    "The below cell uses the [Boto3 SDK](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_training_job) to kick off the training job using both our training set and validation set. Not that the objective is set to 'binary:logistic' which trains a model to output a probability between 0 and 1 (here the probability of a tumor being malignant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "nbpresent": {
     "id": "f3b125ad-a2d5-464c-8cfa-bd203034eee4"
    },
    "ExecuteTime": {
     "end_time": "2023-12-18T16:58:15.158694869Z",
     "start_time": "2023-12-18T16:53:56.548566484Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training job xgb-2023-12-18-16-53-56\n",
      "Training artifacts will be uploaded to: s3://turin-complete-experiments/breast-cancer/output/xgb-2023-12-18-16-53-56\n",
      "InProgress\n",
      "Training job ended with status: Completed\n",
      "CPU times: user 524 ms, sys: 34.4 ms, total: 559 ms\n",
      "Wall time: 4min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from time import gmtime, strftime\n",
    "from sagemaker.image_uris import retrieve\n",
    "\n",
    "job_name = \"xgb-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(\"Training job\", job_name)\n",
    "\n",
    "image = retrieve(framework=\"xgboost\", region=boto3.Session().region_name, version=\"latest\")\n",
    "\n",
    "output_location = \"s3://{}/{}/output/{}\".format(bucket, prefix, job_name)\n",
    "print(\"Training artifacts will be uploaded to: {}\".format(output_location))\n",
    "\n",
    "create_training_params = {\n",
    "    \"AlgorithmSpecification\": {\"TrainingImage\": image, \"TrainingInputMode\": \"File\"},\n",
    "    \"RoleArn\": role,\n",
    "    \"OutputDataConfig\": {\"S3OutputPath\": output_location},\n",
    "    \"ResourceConfig\": {\"InstanceCount\": 1, \"InstanceType\": \"ml.m5.4xlarge\", \"VolumeSizeInGB\": 50},\n",
    "    \"TrainingJobName\": job_name,\n",
    "    \"HyperParameters\": {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"max_depth\": \"5\",\n",
    "        \"eta\": \"0.2\",\n",
    "        \"gamma\": \"4\",\n",
    "        \"min_child_weight\": \"6\",\n",
    "        \"subsample\": \"0.8\",\n",
    "        \"silent\": \"0\",\n",
    "        \"num_round\": \"100\",\n",
    "    },\n",
    "    \"StoppingCondition\": {\"MaxRuntimeInSeconds\": 60 * 60},\n",
    "    \"InputDataConfig\": [\n",
    "        {\n",
    "            \"ChannelName\": \"train\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": \"s3://{}/{}/train\".format(bucket, prefix),\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "                }\n",
    "            },\n",
    "            \"CompressionType\": \"None\",\n",
    "            \"RecordWrapperType\": \"None\",\n",
    "            \"ContentType\": \"text/csv\",\n",
    "        },\n",
    "        {\n",
    "            \"ChannelName\": \"validation\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": \"s3://{}/{}/validation\".format(bucket, prefix),\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "                }\n",
    "            },\n",
    "            \"CompressionType\": \"None\",\n",
    "            \"RecordWrapperType\": \"None\",\n",
    "            \"ContentType\": \"text/csv\",\n",
    "        },\n",
    "    ],\n",
    "}\n",
    "\n",
    "sagemaker = boto3.client(\"sagemaker\")\n",
    "sagemaker.create_training_job(**create_training_params)\n",
    "status = sagemaker.describe_training_job(TrainingJobName=job_name)[\"TrainingJobStatus\"]\n",
    "print(status)\n",
    "\n",
    "try:\n",
    "    sagemaker.get_waiter(\"training_job_completed_or_stopped\").wait(TrainingJobName=job_name)\n",
    "finally:\n",
    "    status = sagemaker.describe_training_job(TrainingJobName=job_name)[\"TrainingJobStatus\"]\n",
    "    print(\"Training job ended with status: \" + status)\n",
    "    if status == \"Failed\":\n",
    "        message = sagemaker.describe_training_job(TrainingJobName=job_name)[\"FailureReason\"]\n",
    "        print(\"Training failed with the following error: {}\".format(message))\n",
    "        raise Exception(\"Training job failed\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's create a model based on our training job. \n",
    "\n",
    "The below cell creates a model in SageMaker based on the training job we just executed. The model can later be deployed using the SageMaker hosting services or in our case used in a Batch Transform job.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T17:01:02.803287122Z",
     "start_time": "2023-12-18T17:00:55.667123283Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgb-2023-12-18-16-53-56\n",
      "arn:aws:sagemaker:us-east-1:898487103080:model/xgb-2023-12-18-16-53-56\n",
      "CPU times: user 173 ms, sys: 4.01 ms, total: 177 ms\n",
      "Wall time: 7.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_name = job_name\n",
    "print(model_name)\n",
    "\n",
    "info = sagemaker.describe_training_job(TrainingJobName=job_name)\n",
    "model_data = info[\"ModelArtifacts\"][\"S3ModelArtifacts\"]\n",
    "\n",
    "primary_container = {\"Image\": image, \"ModelDataUrl\": model_data}\n",
    "\n",
    "create_model_response = sagemaker.create_model(\n",
    "    ModelName=model_name, ExecutionRoleArn=role, PrimaryContainer=primary_container\n",
    ")\n",
    "\n",
    "print(create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "397fb60a-c48b-453f-88ea-4d832b70c919"
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Batch Transform\n",
    "\n",
    "In SageMaker Batch Transform, we introduced a new attribute called __DataProcessing__.In the below cell, we use the [Boto3 SDK](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_transform_job) to kick-off several Batch Transform jobs using different configurations of DataProcessing. Please refer to [Associate Prediction Results with Input Records](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform-data-processing.html) to learn more about how to utilize the __DataProcessing__ attribute.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Without data processing\n",
    "Let's first set the data processing fields to null and inspect the inference results. We'll use it as a baseline to compare to the results with data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T17:09:27.091327258Z",
     "start_time": "2023-12-18T17:01:39.531160873Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Transform job with name:  Batch-Transform-2023-12-18-17-01-39\n",
      "Transform job ended with status: Completed\n",
      "CPU times: user 1.1 s, sys: 43.4 ms, total: 1.15 s\n",
      "Wall time: 7min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from time import gmtime, strftime\n",
    "\n",
    "batch_job_name = \"Batch-Transform-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "input_location = \"s3://{}/{}/batch/{}\".format(\n",
    "    bucket, prefix, batch_file_noID\n",
    ")  # use input data without ID column\n",
    "output_location = \"s3://{}/{}/output/{}\".format(bucket, prefix, batch_job_name)\n",
    "\n",
    "request = {\n",
    "    \"TransformJobName\": batch_job_name,\n",
    "    \"ModelName\": job_name,\n",
    "    \"TransformOutput\": {\n",
    "        \"S3OutputPath\": output_location,\n",
    "        \"Accept\": \"text/csv\",\n",
    "        \"AssembleWith\": \"Line\",\n",
    "    },\n",
    "    \"TransformInput\": {\n",
    "        \"DataSource\": {\"S3DataSource\": {\"S3DataType\": \"S3Prefix\", \"S3Uri\": input_location}},\n",
    "        \"ContentType\": \"text/csv\",\n",
    "        \"SplitType\": \"Line\",\n",
    "        \"CompressionType\": \"None\",\n",
    "    },\n",
    "    \"TransformResources\": {\"InstanceType\": \"ml.m4.xlarge\", \"InstanceCount\": 1},\n",
    "}\n",
    "\n",
    "sagemaker.create_transform_job(**request)\n",
    "print(\"Created Transform job with name: \", batch_job_name)\n",
    "\n",
    "# Wait until the job finishes\n",
    "try:\n",
    "    sagemaker.get_waiter(\"transform_job_completed_or_stopped\").wait(TransformJobName=batch_job_name)\n",
    "finally:\n",
    "    response = sagemaker.describe_transform_job(TransformJobName=batch_job_name)\n",
    "    status = response[\"TransformJobStatus\"]\n",
    "    print(\"Transform job ended with status: \" + status)\n",
    "    if status == \"Failed\":\n",
    "        message = response[\"FailureReason\"]\n",
    "        print(\"Transform failed with the following error: {}\".format(message))\n",
    "        raise Exception(\"Transform job failed\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the output of the Batch Transform job in S3. It should show the list probabilities of tumors being malignant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T17:45:32.421292807Z",
     "start_time": "2023-12-18T17:45:32.377891611Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def get_csv_output_from_s3(s3uri, batch_file):\n",
    "    file_name = \"{}.out\".format(batch_file)\n",
    "    match = re.match(\"s3://([^/]+)/(.*)\", \"{}/{}\".format(s3uri, file_name))\n",
    "    output_bucket, output_prefix = match.group(1), match.group(2)\n",
    "    s3.download_file(output_bucket, output_prefix, file_name)\n",
    "    return pd.read_csv(file_name, sep=\",\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = get_csv_output_from_s3(output_location, batch_file_noID)\n",
    "output_df.head(8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Join the input and the prediction results \n",
    "Now, let's use the new feature to associate the prediction results with their corresponding input records. We can also use the __InputFilter__ to exclude the ID column easily and there's no need to have a separate file in S3.\n",
    "\n",
    "* Set __InputFilter__ to \"$[1:]\": indicates that we are excluding column 0 (the 'ID') before processing the inferences and keeping everything from column 1 to the last column (all the features or predictors)  \n",
    "  \n",
    "  \n",
    "* Set __JoinSource__ to \"Input\": indicates our desire to join the input data with the inference results  \n",
    "\n",
    "\n",
    "* Leave __OutputFilter__ to default (\"$\"), indicating that the joined input and inference results be will saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T17:52:21.579956998Z",
     "start_time": "2023-12-18T17:45:39.044623135Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Transform job with name:  Batch-Transform-2023-12-18-17-45-39\n",
      "Transform job ended with status: Completed\n",
      "CPU times: user 1.1 s, sys: 27.7 ms, total: 1.13 s\n",
      "Wall time: 6min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "batch_job_name = \"Batch-Transform-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "input_location = \"s3://{}/{}/batch/{}\".format(\n",
    "    bucket, prefix, batch_file\n",
    ")  # use input data with ID column cause InputFilter will filter it out\n",
    "output_location = \"s3://{}/{}/output/{}\".format(bucket, prefix, batch_job_name)\n",
    "\n",
    "request[\"TransformJobName\"] = batch_job_name\n",
    "request[\"TransformInput\"][\"DataSource\"][\"S3DataSource\"][\"S3Uri\"] = input_location\n",
    "request[\"TransformOutput\"][\"S3OutputPath\"] = output_location\n",
    "\n",
    "request[\"DataProcessing\"] = {\n",
    "    \"InputFilter\": \"$[1:]\",  # exclude the ID column (index 0)\n",
    "    \"JoinSource\": \"Input\",  # join the input with the inference results\n",
    "}\n",
    "\n",
    "sagemaker.create_transform_job(**request)\n",
    "print(\"Created Transform job with name: \", batch_job_name)\n",
    "\n",
    "# Wait until the job finishes\n",
    "try:\n",
    "    sagemaker.get_waiter(\"transform_job_completed_or_stopped\").wait(TransformJobName=batch_job_name)\n",
    "finally:\n",
    "    response = sagemaker.describe_transform_job(TransformJobName=batch_job_name)\n",
    "    status = response[\"TransformJobStatus\"]\n",
    "    print(\"Transform job ended with status: \" + status)\n",
    "    if status == \"Failed\":\n",
    "        message = response[\"FailureReason\"]\n",
    "        print(\"Transform failed with the following error: {}\".format(message))\n",
    "        raise Exception(\"Transform job failed\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the output of the Batch Transform job in S3. It should show the list of tumors identified by their original feature columns and their corresponding probabilities of being malignant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T17:54:15.499549315Z",
     "start_time": "2023-12-18T17:54:14.346287367Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "         0       1      2       3       4        5        6        7   \\\n0  84501001  12.460  24.04   83.97   475.9  0.11860  0.23960  0.22730   \n1    845636  16.020  23.24  102.70   797.8  0.08206  0.06669  0.03299   \n2   8510653  13.080  15.71   85.63   520.0  0.10750  0.12700  0.04568   \n3    855133  14.990  25.20   95.54   698.8  0.09387  0.05131  0.02398   \n4    857155  12.050  14.63   78.04   449.3  0.10310  0.09092  0.06592   \n5    859575  18.940  21.31  123.60  1130.0  0.09009  0.10290  0.10800   \n6    859711   8.888  14.64   58.79   244.0  0.09783  0.15310  0.08606   \n7     86355  22.270  19.67  152.80  1509.0  0.13260  0.27680  0.42640   \n\n        8       9   ...     22      23      24       25       26       27  \\\n0  0.08543  0.2030  ...  40.68   97.65   711.4  0.18530  1.05800  1.10500   \n1  0.03323  0.1528  ...  33.88  123.80  1150.0  0.11810  0.15510  0.14590   \n2  0.03110  0.1967  ...  20.49   96.09   630.5  0.13120  0.27760  0.18900   \n3  0.02899  0.1565  ...  25.20   95.54   698.8  0.09387  0.05131  0.02398   \n4  0.02749  0.1675  ...  20.70   89.88   582.6  0.14940  0.21560  0.30500   \n5  0.07951  0.1582  ...  26.58  165.90  1866.0  0.11930  0.23360  0.26870   \n6  0.02872  0.1902  ...  15.67   62.56   284.4  0.12070  0.24360  0.14340   \n7  0.18230  0.2556  ...  28.01  206.80  2360.0  0.17010  0.69970  0.96080   \n\n        28      29       30        31  \n0  0.22100  0.4366  0.20750  0.789157  \n1  0.09975  0.2948  0.08452  0.624410  \n2  0.07283  0.3184  0.08183  0.005715  \n3  0.02899  0.1565  0.05504  0.020026  \n4  0.06548  0.2747  0.08301  0.013134  \n5  0.17890  0.2551  0.06589  0.989036  \n6  0.04786  0.2254  0.10840  0.010582  \n7  0.29100  0.4055  0.09789  0.992965  \n\n[8 rows x 32 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>22</th>\n      <th>23</th>\n      <th>24</th>\n      <th>25</th>\n      <th>26</th>\n      <th>27</th>\n      <th>28</th>\n      <th>29</th>\n      <th>30</th>\n      <th>31</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>84501001</td>\n      <td>12.460</td>\n      <td>24.04</td>\n      <td>83.97</td>\n      <td>475.9</td>\n      <td>0.11860</td>\n      <td>0.23960</td>\n      <td>0.22730</td>\n      <td>0.08543</td>\n      <td>0.2030</td>\n      <td>...</td>\n      <td>40.68</td>\n      <td>97.65</td>\n      <td>711.4</td>\n      <td>0.18530</td>\n      <td>1.05800</td>\n      <td>1.10500</td>\n      <td>0.22100</td>\n      <td>0.4366</td>\n      <td>0.20750</td>\n      <td>0.789157</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>845636</td>\n      <td>16.020</td>\n      <td>23.24</td>\n      <td>102.70</td>\n      <td>797.8</td>\n      <td>0.08206</td>\n      <td>0.06669</td>\n      <td>0.03299</td>\n      <td>0.03323</td>\n      <td>0.1528</td>\n      <td>...</td>\n      <td>33.88</td>\n      <td>123.80</td>\n      <td>1150.0</td>\n      <td>0.11810</td>\n      <td>0.15510</td>\n      <td>0.14590</td>\n      <td>0.09975</td>\n      <td>0.2948</td>\n      <td>0.08452</td>\n      <td>0.624410</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8510653</td>\n      <td>13.080</td>\n      <td>15.71</td>\n      <td>85.63</td>\n      <td>520.0</td>\n      <td>0.10750</td>\n      <td>0.12700</td>\n      <td>0.04568</td>\n      <td>0.03110</td>\n      <td>0.1967</td>\n      <td>...</td>\n      <td>20.49</td>\n      <td>96.09</td>\n      <td>630.5</td>\n      <td>0.13120</td>\n      <td>0.27760</td>\n      <td>0.18900</td>\n      <td>0.07283</td>\n      <td>0.3184</td>\n      <td>0.08183</td>\n      <td>0.005715</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>855133</td>\n      <td>14.990</td>\n      <td>25.20</td>\n      <td>95.54</td>\n      <td>698.8</td>\n      <td>0.09387</td>\n      <td>0.05131</td>\n      <td>0.02398</td>\n      <td>0.02899</td>\n      <td>0.1565</td>\n      <td>...</td>\n      <td>25.20</td>\n      <td>95.54</td>\n      <td>698.8</td>\n      <td>0.09387</td>\n      <td>0.05131</td>\n      <td>0.02398</td>\n      <td>0.02899</td>\n      <td>0.1565</td>\n      <td>0.05504</td>\n      <td>0.020026</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>857155</td>\n      <td>12.050</td>\n      <td>14.63</td>\n      <td>78.04</td>\n      <td>449.3</td>\n      <td>0.10310</td>\n      <td>0.09092</td>\n      <td>0.06592</td>\n      <td>0.02749</td>\n      <td>0.1675</td>\n      <td>...</td>\n      <td>20.70</td>\n      <td>89.88</td>\n      <td>582.6</td>\n      <td>0.14940</td>\n      <td>0.21560</td>\n      <td>0.30500</td>\n      <td>0.06548</td>\n      <td>0.2747</td>\n      <td>0.08301</td>\n      <td>0.013134</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>859575</td>\n      <td>18.940</td>\n      <td>21.31</td>\n      <td>123.60</td>\n      <td>1130.0</td>\n      <td>0.09009</td>\n      <td>0.10290</td>\n      <td>0.10800</td>\n      <td>0.07951</td>\n      <td>0.1582</td>\n      <td>...</td>\n      <td>26.58</td>\n      <td>165.90</td>\n      <td>1866.0</td>\n      <td>0.11930</td>\n      <td>0.23360</td>\n      <td>0.26870</td>\n      <td>0.17890</td>\n      <td>0.2551</td>\n      <td>0.06589</td>\n      <td>0.989036</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>859711</td>\n      <td>8.888</td>\n      <td>14.64</td>\n      <td>58.79</td>\n      <td>244.0</td>\n      <td>0.09783</td>\n      <td>0.15310</td>\n      <td>0.08606</td>\n      <td>0.02872</td>\n      <td>0.1902</td>\n      <td>...</td>\n      <td>15.67</td>\n      <td>62.56</td>\n      <td>284.4</td>\n      <td>0.12070</td>\n      <td>0.24360</td>\n      <td>0.14340</td>\n      <td>0.04786</td>\n      <td>0.2254</td>\n      <td>0.10840</td>\n      <td>0.010582</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>86355</td>\n      <td>22.270</td>\n      <td>19.67</td>\n      <td>152.80</td>\n      <td>1509.0</td>\n      <td>0.13260</td>\n      <td>0.27680</td>\n      <td>0.42640</td>\n      <td>0.18230</td>\n      <td>0.2556</td>\n      <td>...</td>\n      <td>28.01</td>\n      <td>206.80</td>\n      <td>2360.0</td>\n      <td>0.17010</td>\n      <td>0.69970</td>\n      <td>0.96080</td>\n      <td>0.29100</td>\n      <td>0.4055</td>\n      <td>0.09789</td>\n      <td>0.992965</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 32 columns</p>\n</div>"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df = get_csv_output_from_s3(output_location, batch_file)\n",
    "output_df.head(8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Update the output filter to keep only ID and prediction results\n",
    "Let's try to change the output filter  to \"$[0,-1]\", indicating that when presenting the output, we only want to keep column 0 (the 'ID') and the last column (the inference result i.e. the probability of a given tumor to be malignant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T18:02:41.495363806Z",
     "start_time": "2023-12-18T17:55:59.561316435Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Transform job with name:  Batch-Transform-2023-12-18-17-55-59\n",
      "Transform job ended with status: Completed\n",
      "CPU times: user 960 ms, sys: 34 ms, total: 994 ms\n",
      "Wall time: 6min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "batch_job_name = \"Batch-Transform-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "output_location = \"s3://{}/{}/output/{}\".format(bucket, prefix, batch_job_name)\n",
    "\n",
    "request[\"TransformJobName\"] = batch_job_name\n",
    "request[\"TransformOutput\"][\"S3OutputPath\"] = output_location\n",
    "request[\"DataProcessing\"][\n",
    "    \"OutputFilter\"\n",
    "] = \"$[0, -1]\"  # keep the first and last column of the joined output\n",
    "\n",
    "sagemaker.create_transform_job(**request)\n",
    "print(\"Created Transform job with name: \", batch_job_name)\n",
    "\n",
    "# Wait until the job finishes\n",
    "try:\n",
    "    sagemaker.get_waiter(\"transform_job_completed_or_stopped\").wait(TransformJobName=batch_job_name)\n",
    "finally:\n",
    "    response = sagemaker.describe_transform_job(TransformJobName=batch_job_name)\n",
    "    status = response[\"TransformJobStatus\"]\n",
    "    print(\"Transform job ended with status: \" + status)\n",
    "    if status == \"Failed\":\n",
    "        message = response[\"FailureReason\"]\n",
    "        print(\"Transform failed with the following error: {}\".format(message))\n",
    "        raise Exception(\"Transform job failed\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's inspect the output of the Batch Transform job in S3 again. It should show 2 columns: the ID and their corresponding probabilities of being malignant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T18:30:19.370695031Z",
     "start_time": "2023-12-18T18:30:18.150732342Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "          0         1\n0  84501001  0.789157\n1    845636  0.624410\n2   8510653  0.005715\n3    855133  0.020026\n4    857155  0.013134\n5    859575  0.989036\n6    859711  0.010582\n7     86355  0.992965",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>84501001</td>\n      <td>0.789157</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>845636</td>\n      <td>0.624410</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8510653</td>\n      <td>0.005715</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>855133</td>\n      <td>0.020026</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>857155</td>\n      <td>0.013134</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>859575</td>\n      <td>0.989036</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>859711</td>\n      <td>0.010582</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>86355</td>\n      <td>0.992965</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df = get_csv_output_from_s3(output_location, batch_file)\n",
    "output_df.head(8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, we can use DataProcessing to \n",
    "1. Filter / select useful features from the input dataset. e.g. exclude ID columns.\n",
    "2. Associate the prediction results with their corresponding input records.\n",
    "3. Filter the original or joined results before saving to S3. e.g. keep ID and probability columns only."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-1/sagemaker_batch_transform|batch_transform_associate_predictions_with_input|Batch Transform - breast cancer prediction with lowel level SDK.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-2/sagemaker_batch_transform|batch_transform_associate_predictions_with_input|Batch Transform - breast cancer prediction with lowel level SDK.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-1/sagemaker_batch_transform|batch_transform_associate_predictions_with_input|Batch Transform - breast cancer prediction with lowel level SDK.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ca-central-1/sagemaker_batch_transform|batch_transform_associate_predictions_with_input|Batch Transform - breast cancer prediction with lowel level SDK.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/sa-east-1/sagemaker_batch_transform|batch_transform_associate_predictions_with_input|Batch Transform - breast cancer prediction with lowel level SDK.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-1/sagemaker_batch_transform|batch_transform_associate_predictions_with_input|Batch Transform - breast cancer prediction with lowel level SDK.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-2/sagemaker_batch_transform|batch_transform_associate_predictions_with_input|Batch Transform - breast cancer prediction with lowel level SDK.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-3/sagemaker_batch_transform|batch_transform_associate_predictions_with_input|Batch Transform - breast cancer prediction with lowel level SDK.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-central-1/sagemaker_batch_transform|batch_transform_associate_predictions_with_input|Batch Transform - breast cancer prediction with lowel level SDK.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-north-1/sagemaker_batch_transform|batch_transform_associate_predictions_with_input|Batch Transform - breast cancer prediction with lowel level SDK.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-1/sagemaker_batch_transform|batch_transform_associate_predictions_with_input|Batch Transform - breast cancer prediction with lowel level SDK.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-2/sagemaker_batch_transform|batch_transform_associate_predictions_with_input|Batch Transform - breast cancer prediction with lowel level SDK.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-1/sagemaker_batch_transform|batch_transform_associate_predictions_with_input|Batch Transform - breast cancer prediction with lowel level SDK.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-2/sagemaker_batch_transform|batch_transform_associate_predictions_with_input|Batch Transform - breast cancer prediction with lowel level SDK.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-south-1/sagemaker_batch_transform|batch_transform_associate_predictions_with_input|Batch Transform - breast cancer prediction with lowel level SDK.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the License). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the license file accompanying this file. This file is distributed on an AS IS BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
